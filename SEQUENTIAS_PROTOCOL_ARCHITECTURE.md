# üåç Sequentias Protocol Architecture
## Decentralized Genomics Network with Global Node Orchestration

**Vision**: Every GenoBank API/microservice can be replicated as a node in the Sequentias Network, enabling truly decentralized genomics with patient-owned data AND distributed compute.

---

## üéØ The Real Problem We're Solving

### Current Architecture (Centralized):
```
User ‚Üí api.genobank.app (Single Linux Server) ‚Üí S3
         ‚Üë
    All jobs execute HERE
    (CPU/GPU bottleneck)
```

### Sequentias Protocol Architecture (Decentralized):
```
User ‚Üí Sequentias Network ‚Üí Optimal Node
         ‚Üì
    Global Registry:
    - Mexico Node (CPU, low latency for Mexico users)
    - California Node (GPU, for AlphaGenome jobs)
    - Europe Node (GDPR compliance, EU data)
    - Brazil Node (cached replicas for SA users)

    Each Node = Full GenoBank API + Bioinformatics + Story Protocol
```

---

## üèóÔ∏è Three-Layer Architecture: FUSE + BioNFS + Sequentias

### Layer 1: FUSE (Client Experience Layer)
**Purpose**: User sees distributed files as if they're local

```bash
# User experience (same worldwide)
biofs mount ~/genomics
ls ~/genomics/
# Files appear local, but are actually distributed across global nodes!

biofs analyze sample.vcf --pipeline alphagenome
# Job automatically routes to optimal node (GPU required? ‚Üí California)
```

**Implementation**: `/home/ubuntu/web3fuse/` (existing codebase)
- Mounts distributed filesystem locally
- Translates file operations to BioNFS protocol calls
- Handles session management (24-hour Web3 sessions)

### Layer 2: BioNFS Protocol (Network Communication Layer)
**Purpose**: Standardized protocol for node-to-node communication

```
Node-to-Node Communication:
1. Node Discovery (find available nodes)
2. Job Submission (send work to optimal node)
3. Data Replication (cache files at edge)
4. Result Streaming (return output to client)
```

**Implementation**: New BioNFS server (FastAPI + gRPC)
- HTTP/2 for client connections
- gRPC for node-to-node communication
- WebSocket for real-time job updates

### Layer 3: Sequentias Network (Orchestration Layer)
**Purpose**: Smart routing, resource allocation, fault tolerance

```javascript
// Sequentias Protocol decides:
const optimalNode = sequentias.findNode({
    userLocation: "Mexico City",
    jobType: "alphagenome",  // Needs GPU
    dataLocation: "California S3",
    constraints: {
        maxLatency: 100,  // ms
        maxCost: 10,      // USD
        requireGPU: true
    }
});

// Result: Routes to Austin, Texas node
// Why? Middle ground: Has GPU, close to both user and data
```

---

## üåê Sequentias Node Architecture

### What is a Sequentias Node?

Each node is a **full replica** of current GenoBank infrastructure:

```
Sequentias Node (Mexico City):
‚îú‚îÄ‚îÄ BioNFS Server (network protocol)
‚îú‚îÄ‚îÄ GenoBank APIs
‚îÇ   ‚îú‚îÄ‚îÄ api_bioip (tokenization)
‚îÇ   ‚îú‚îÄ‚îÄ api_vcf_annotator (OpenCRAVAT)
‚îÇ   ‚îú‚îÄ‚îÄ api_alphagenome (variant scoring)
‚îÇ   ‚îú‚îÄ‚îÄ api_somos_dao (ancestry)
‚îÇ   ‚îú‚îÄ‚îÄ api_clara (GPU variant calling)
‚îÇ   ‚îî‚îÄ‚îÄ api_trio (family analysis)
‚îú‚îÄ‚îÄ Bioinformatics Tools
‚îÇ   ‚îú‚îÄ‚îÄ DeepVariant (GPU)
‚îÇ   ‚îú‚îÄ‚îÄ OpenCRAVAT (annotation)
‚îÇ   ‚îú‚îÄ‚îÄ bcftools, samtools, etc.
‚îú‚îÄ‚îÄ AI Models
‚îÇ   ‚îú‚îÄ‚îÄ AlphaGenome (local inference)
‚îÇ   ‚îú‚îÄ‚îÄ Claude API integration
‚îÇ   ‚îî‚îÄ‚îÄ Gemini API integration
‚îú‚îÄ‚îÄ Vault Storage
‚îÇ   ‚îú‚îÄ‚îÄ Local cache (hot data)
‚îÇ   ‚îú‚îÄ‚îÄ S3 connection (cold data)
‚îÇ   ‚îî‚îÄ‚îÄ IPFS pinning (NFT metadata)
‚îî‚îÄ‚îÄ Story Protocol Client
    ‚îú‚îÄ‚îÄ License verification
    ‚îú‚îÄ‚îÄ IP registration
    ‚îî‚îÄ‚îÄ PIL enforcement
```

### Node Capabilities Matrix:

| Node Type | CPU | GPU | Storage | Use Cases |
|-----------|-----|-----|---------|-----------|
| **Basic** | ‚úÖ | ‚ùå | 1TB | VCF annotation, ancestry analysis |
| **GPU** | ‚úÖ | ‚úÖ | 5TB | AlphaGenome, DeepVariant, Clara |
| **Storage** | ‚úÖ | ‚ùå | 50TB | Data replication, edge caching |
| **AI** | ‚úÖ | ‚úÖ | 10TB | Claude analysis, AlphaGenome |

---

## üöÄ The Hybrid Answer: BOTH FUSE + NFS

### Why FUSE Alone Won't Work:
```
‚ùå FUSE is client-side only
   - Can't route jobs to different nodes
   - No node-to-node communication
   - Each client needs full implementation

‚ùå No network protocol
   - Nodes can't discover each other
   - Can't distribute workload
   - No fault tolerance (one node down = job fails)
```

### Why NFS-like Protocol Alone Won't Work:
```
‚ùå Requires mounting on every client
   - User must configure NFS mounts
   - Complex setup (not "just works")
   - Doesn't feel like local files

‚ùå Not suitable for CLI tools
   - bcftools, samtools expect local files
   - Performance overhead for small operations
   - Doesn't integrate with existing workflows
```

### ‚úÖ The Hybrid Solution: FUSE + BioNFS

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  CLIENT SIDE (User's Computer)                      ‚îÇ
‚îÇ                                                     ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îÇ
‚îÇ  ‚îÇ  FUSE Layer (web3fuse)                       ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ  - Mounts: /mnt/genomics/                    ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ  - User sees files as local                  ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ  - Works with bcftools, samtools, IGV       ‚îÇ  ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îÇ
‚îÇ                 ‚îÇ                                    ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îÇ
‚îÇ  ‚îÇ  BioNFS Client Library                       ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ  - Protocol implementation                   ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ  - Session management                        ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ  - Node discovery                            ‚îÇ  ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                  ‚îÇ
            BioNFS Protocol
          (HTTP/2 + gRPC + WebSocket)
                  ‚îÇ
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  SEQUENTIAS NETWORK (Global Distributed Nodes)       ‚îÇ
‚îÇ                                                       ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îÇ
‚îÇ  ‚îÇ Mexico Node  ‚îÇ  ‚îÇ California   ‚îÇ  ‚îÇ Europe     ‚îÇ ‚îÇ
‚îÇ  ‚îÇ (Basic)      ‚îÇ  ‚îÇ (GPU)        ‚îÇ  ‚îÇ (GDPR)     ‚îÇ ‚îÇ
‚îÇ  ‚îÇ              ‚îÇ  ‚îÇ              ‚îÇ  ‚îÇ            ‚îÇ ‚îÇ
‚îÇ  ‚îÇ BioNFS Server‚îÇ  ‚îÇ BioNFS Server‚îÇ  ‚îÇ BioNFS    ‚îÇ ‚îÇ
‚îÇ  ‚îÇ Annotation   ‚îÇ  ‚îÇ AlphaGenome  ‚îÇ  ‚îÇ Server     ‚îÇ ‚îÇ
‚îÇ  ‚îÇ Ancestry     ‚îÇ  ‚îÇ DeepVariant  ‚îÇ  ‚îÇ Storage    ‚îÇ ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îÇ
‚îÇ                                                       ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îÇ
‚îÇ  ‚îÇ  Sequentias Orchestrator (Smart Routing)        ‚îÇ ‚îÇ
‚îÇ  ‚îÇ  - Find optimal node based on:                  ‚îÇ ‚îÇ
‚îÇ  ‚îÇ    ‚Ä¢ User location (latency)                    ‚îÇ ‚îÇ
‚îÇ  ‚îÇ    ‚Ä¢ Job requirements (CPU/GPU)                 ‚îÇ ‚îÇ
‚îÇ  ‚îÇ    ‚Ä¢ Data location (bandwidth)                  ‚îÇ ‚îÇ
‚îÇ  ‚îÇ    ‚Ä¢ Cost constraints (budget)                  ‚îÇ ‚îÇ
‚îÇ  ‚îÇ    ‚Ä¢ Node availability (load balancing)         ‚îÇ ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

---

## üìã Implementation Roadmap

### Week 1-2: FUSE Client Enhancement
**Update `/home/ubuntu/web3fuse/`**

```c
// web3fuse with Sequentias support
static int bionfs_fuse_open(const char *path, struct fuse_file_info *fi) {
    // 1. Parse IP Asset ID from path
    char *ip_id = parse_ip_from_path(path);

    // 2. Query Sequentias registry for optimal node
    sequentias_node_t *node = sequentias_find_optimal_node(
        current_user_location,
        ip_id,
        /* requires_gpu */ false
    );

    // 3. Connect to node via BioNFS protocol
    bionfs_connection_t *conn = bionfs_connect(node->endpoint);

    // 4. Authenticate with session
    bionfs_auth(conn, user_session);

    // 5. Open file on remote node
    bionfs_file_t *file = bionfs_remote_open(conn, ip_id);
    fi->fh = (uint64_t)file;

    return 0;
}

static int bionfs_fuse_read(const char *path, char *buf, size_t size, off_t offset,
                            struct fuse_file_info *fi) {
    bionfs_file_t *file = (bionfs_file_t *)fi->fh;

    // Stream from remote node (Mexico, California, wherever optimal)
    return bionfs_remote_read(file->connection, file->handle, buf, size, offset);
}
```

### Week 3-4: BioNFS Protocol Server
**New: `/home/ubuntu/sequentias_node/bionfs_server/`**

```python
from fastapi import FastAPI, WebSocket
from grpc import aio
import sequentias_pb2  # Protocol Buffers

app = FastAPI()

class BioNFSServer:
    def __init__(self):
        self.node_id = os.environ['SEQUENTIAS_NODE_ID']
        self.location = os.environ['NODE_LOCATION']  # "Mexico City"
        self.has_gpu = torch.cuda.is_available()
        self.registry = SequentiasRegistry()

    async def register_with_network(self):
        """Register this node with Sequentias Network"""
        await self.registry.register_node({
            "node_id": self.node_id,
            "location": self.location,
            "endpoint": "https://mexico.sequentias.io",
            "capabilities": {
                "cpu_cores": 64,
                "gpu": "NVIDIA A100" if self.has_gpu else None,
                "storage_tb": 10,
                "services": [
                    "vcf_annotation",
                    "ancestry_analysis",
                    "alphagenome" if self.has_gpu else None
                ]
            },
            "operator": os.environ['NODE_OPERATOR_WALLET']
        })

    @app.post("/bionfs/v1/execute_job")
    async def execute_job(job_spec: JobSpec):
        """Execute job on this node"""
        # 1. Verify session
        session = verify_session(job_spec.session_token)

        # 2. Check if file is cached locally
        if not self.cache.has(job_spec.ip_id):
            # Fetch from primary vault
            await self.replicate_file(job_spec.ip_id)

        # 3. Execute pipeline
        if job_spec.pipeline == "alphagenome":
            if not self.has_gpu:
                return {"error": "This node has no GPU, routing error!"}
            result = await run_alphagenome(job_spec.ip_id)
        elif job_spec.pipeline == "rare_coding":
            result = await run_opencravat(job_spec.ip_id, "rare_coding")

        # 4. Replicate result to user's preferred vault
        await self.replicate_result(result, session.wallet)

        # 5. Return presigned URL for download
        return {
            "status": "success",
            "result_ip_id": result.ip_id,
            "download_url": self.generate_download_url(result.ip_id)
        }

# gRPC service for node-to-node communication
class NodeCommunication(sequentias_pb2_grpc.NodeServicer):
    async def ReplicateFile(self, request, context):
        """Another node requesting file replication"""
        # Transfer file to requesting node
        file_data = await self.vault.get_file(request.ip_id)
        return sequentias_pb2.FileData(
            ip_id=request.ip_id,
            data=file_data,
            checksum=hashlib.sha256(file_data).hexdigest()
        )
```

### Week 5-6: Sequentias Registry (Smart Contract)
**Deploy on Story Protocol**

```solidity
// SPDX-License-Identifier: MIT
pragma solidity ^0.8.20;

contract SequentiasRegistry {
    struct Node {
        address operator;
        string location;        // "Mexico City", "Palo Alto", "Berlin"
        string endpoint;        // "https://mexico.sequentias.io"
        bool hasGPU;
        uint256 cpuCores;
        uint256 storageTB;
        uint256 uptime;         // Percentage (9500 = 95.00%)
        uint256 jobsCompleted;
        bool active;
    }

    struct Job {
        bytes32 jobId;
        address requester;
        address executingNode;
        string pipeline;        // "alphagenome", "rare_coding"
        bytes32 ipAssetId;
        uint256 startTime;
        uint256 endTime;
        JobStatus status;
    }

    enum JobStatus { Pending, Running, Completed, Failed }

    mapping(address => Node) public nodes;
    mapping(bytes32 => Job) public jobs;

    event NodeRegistered(address indexed operator, string location);
    event JobRouted(bytes32 indexed jobId, address indexed node);
    event JobCompleted(bytes32 indexed jobId, uint256 executionTime);

    function registerNode(
        string memory location,
        string memory endpoint,
        bool hasGPU,
        uint256 cpuCores,
        uint256 storageTB
    ) external {
        nodes[msg.sender] = Node({
            operator: msg.sender,
            location: location,
            endpoint: endpoint,
            hasGPU: hasGPU,
            cpuCores: cpuCores,
            storageTB: storageTB,
            uptime: 10000,  // Start at 100%
            jobsCompleted: 0,
            active: true
        });

        emit NodeRegistered(msg.sender, location);
    }

    function findOptimalNode(
        string memory userLocation,
        bool requiresGPU,
        string memory dataLocation
    ) external view returns (address) {
        // Scoring algorithm:
        // 1. Location proximity (30%)
        // 2. Resource availability (30%)
        // 3. Uptime/reliability (20%)
        // 4. Data proximity (20%)

        address bestNode;
        uint256 bestScore = 0;

        for (uint i = 0; i < getAllNodes().length; i++) {
            address nodeAddr = getAllNodes()[i];
            Node memory node = nodes[nodeAddr];

            if (!node.active) continue;
            if (requiresGPU && !node.hasGPU) continue;

            uint256 score = calculateNodeScore(
                node,
                userLocation,
                dataLocation
            );

            if (score > bestScore) {
                bestScore = score;
                bestNode = nodeAddr;
            }
        }

        return bestNode;
    }

    function submitJob(
        bytes32 jobId,
        address nodeOperator,
        string memory pipeline,
        bytes32 ipAssetId
    ) external {
        jobs[jobId] = Job({
            jobId: jobId,
            requester: msg.sender,
            executingNode: nodeOperator,
            pipeline: pipeline,
            ipAssetId: ipAssetId,
            startTime: block.timestamp,
            endTime: 0,
            status: JobStatus.Running
        });

        emit JobRouted(jobId, nodeOperator);
    }

    function completeJob(bytes32 jobId) external {
        Job storage job = jobs[jobId];
        require(job.executingNode == nodes[msg.sender].operator, "Not executor");

        job.endTime = block.timestamp;
        job.status = JobStatus.Completed;

        // Update node stats
        nodes[msg.sender].jobsCompleted++;

        emit JobCompleted(jobId, job.endTime - job.startTime);
    }
}
```

### Week 7-8: Orchestration Logic
**New: `/home/ubuntu/sequentias_node/orchestrator/`**

```python
from web3 import Web3
from geopy.distance import geodesic

class SequentiasOrchestrator:
    def __init__(self):
        self.w3 = Web3(Web3.HTTPProvider(os.environ['STORY_RPC_URL']))
        self.registry = self.w3.eth.contract(
            address=SEQUENTIAS_REGISTRY_ADDRESS,
            abi=SEQUENTIAS_REGISTRY_ABI
        )

    async def route_job(self, job_spec: JobSpec) -> Node:
        """
        Find optimal node for job execution

        Considers:
        1. User location (minimize latency)
        2. Data location (minimize bandwidth)
        3. Resource requirements (CPU/GPU)
        4. Cost constraints (budget)
        5. Node reputation (uptime, success rate)
        """

        # Get user's location (from IP or explicitly provided)
        user_location = await self.get_user_location(job_spec.user_ip)

        # Get data location (where is the IP Asset stored?)
        data_location = await self.get_data_location(job_spec.ip_id)

        # Query registry for available nodes
        all_nodes = await self.get_all_active_nodes()

        # Score each node
        scored_nodes = []
        for node in all_nodes:
            score = await self.calculate_node_score(
                node=node,
                user_location=user_location,
                data_location=data_location,
                requires_gpu=job_spec.pipeline in ["alphagenome", "deepvariant"],
                budget=job_spec.max_cost
            )
            scored_nodes.append((score, node))

        # Sort by score (highest first)
        scored_nodes.sort(reverse=True, key=lambda x: x[0])

        # Return best node
        return scored_nodes[0][1]

    async def calculate_node_score(
        self,
        node: Node,
        user_location: Location,
        data_location: Location,
        requires_gpu: bool,
        budget: float
    ) -> float:
        """
        Scoring algorithm:
        - Latency score (30%): Distance from user
        - Bandwidth score (20%): Distance from data
        - Resource score (30%): Has required resources?
        - Reliability score (10%): Uptime percentage
        - Cost score (10%): Within budget?
        """

        # Latency score (closer to user = better)
        user_distance = geodesic(
            (user_location.lat, user_location.lon),
            (node.location.lat, node.location.lon)
        ).kilometers
        latency_score = max(0, 100 - (user_distance / 100))  # 0-100

        # Bandwidth score (closer to data = better)
        data_distance = geodesic(
            (data_location.lat, data_location.lon),
            (node.location.lat, node.location.lon)
        ).kilometers
        bandwidth_score = max(0, 100 - (data_distance / 100))

        # Resource score
        if requires_gpu and not node.has_gpu:
            return 0  # Disqualify immediately
        resource_score = 100 if node.has_gpu or not requires_gpu else 0

        # Reliability score
        reliability_score = node.uptime / 100  # Convert to 0-100

        # Cost score
        estimated_cost = await self.estimate_job_cost(node, requires_gpu)
        cost_score = 100 if estimated_cost <= budget else 0

        # Weighted average
        total_score = (
            latency_score * 0.3 +
            bandwidth_score * 0.2 +
            resource_score * 0.3 +
            reliability_score * 0.1 +
            cost_score * 0.1
        )

        return total_score

    async def execute_job_on_node(self, node: Node, job_spec: JobSpec):
        """Submit job to selected node via BioNFS protocol"""

        async with aiohttp.ClientSession() as session:
            # Submit job to node
            response = await session.post(
                f"{node.endpoint}/bionfs/v1/execute_job",
                json={
                    "session_token": job_spec.session_token,
                    "ip_id": job_spec.ip_id,
                    "pipeline": job_spec.pipeline,
                    "parameters": job_spec.parameters
                }
            )

            result = await response.json()

            # Register job completion on blockchain
            tx = self.registry.functions.completeJob(
                jobId=job_spec.job_id
            ).build_transaction({
                'from': node.operator,
                'nonce': self.w3.eth.get_transaction_count(node.operator)
            })

            # Node operator signs and submits
            # ...

            return result
```

---

## üåç Real-World Usage Scenarios

### Scenario 1: Researcher in Mexico Analyzes Local VCF
```bash
# User in Mexico City
biofs login
biofs mount ~/genomics

# File is in California S3
ls ~/genomics/granted/
# 0xCCe1.../father.vcf (152 MB, California vault)

# Analyze with rare_coding pipeline
biofs analyze ~/genomics/granted/0xCCe1.../father.vcf --pipeline rare_coding

# Behind the scenes:
# 1. Sequentias finds optimal node:
#    - Mexico City node (50ms latency, no GPU needed)
# 2. Mexico node caches file from California (one-time transfer)
# 3. Job executes on Mexico node
# 4. Results streamed back to user
# Total time: 30 seconds (vs 5 minutes if run in California)
```

### Scenario 2: Researcher in Palo Alto Needs GPU
```bash
# User in Palo Alto, California
biofs login
biofs mount ~/genomics

# Same file (in California S3)
# Analyze with AlphaGenome (needs GPU)
biofs analyze ~/genomics/granted/0xCCe1.../father.vcf --pipeline alphagenome

# Behind the scenes:
# 1. Sequentias finds optimal node:
#    - San Francisco GPU node (30 miles away, has A100)
# 2. SF node already has file cached (same region)
# 3. Job executes on GPU
# 4. Results in <10 minutes (GPU accelerated)
```

### Scenario 3: Patient in Brazil Downloads Own Data
```bash
# Patient in S√£o Paulo (owns data, no license needed)
biofs login  # Authenticate as owner
biofs mount ~/my-genomics

# Their files appear
ls ~/my-genomics/
# my_exome.vcf (owned file)

# Download to local
biofs download ~/my-genomics/my_exome.vcf ./local_copy.vcf

# Behind the scenes:
# 1. Sequentias checks: Brazil node has cached copy?
#    - No ‚Üí Replicate from California vault to Brazil node
#    - Yes ‚Üí Stream from Brazil node
# 2. Future access: Instant (cached in Brazil)
# 3. Latency: 20ms (vs 200ms from California)
```

### Scenario 4: GPU Jobs with Data in Different Regions
```bash
# User in Tokyo needs GPU for DeepVariant
# Data is in EU (GDPR-compliant storage)

biofs analyze eu_patient_data.fastq --pipeline deepvariant

# Behind the scenes:
# 1. Sequentias scoring:
#    - Tokyo GPU node: High latency to EU data (disqualified)
#    - Singapore GPU node: Medium latency to EU + Tokyo (optimal)
#    - EU GPU node: High latency to Tokyo user (suboptimal)
# 2. Routes to Singapore node (middle ground)
# 3. Singapore caches EU data
# 4. Executes with GPU
# 5. Results stream to Tokyo (low latency)
```

---

## üîê Security & Licensing in Distributed Network

### License Token Verification Across Nodes

```python
# Every node verifies licenses independently
def verify_license_on_node(ip_id: str, wallet: str) -> bool:
    """
    Each node queries Story Protocol directly
    No central authority needed!
    """

    # Query local MongoDB cache first (fast)
    local_license = license_cache.find_one({
        "ip_id": ip_id,
        "receiver": wallet,
        "status": "active"
    })

    if local_license:
        # Verify cache is fresh (< 1 hour old)
        if local_license['cached_at'] > time.time() - 3600:
            return True

    # If no cache or stale, verify on blockchain
    license_token_contract = story_protocol.get_contract("LicenseToken")
    owner = license_token_contract.functions.ownerOf(local_license['license_token_id']).call()

    if owner.lower() == wallet.lower():
        # Update cache
        license_cache.update_one(
            {"_id": local_license['_id']},
            {"$set": {"cached_at": time.time()}}
        )
        return True

    return False
```

### Data Replication with License Enforcement

```python
# Node can only replicate files it's authorized to serve
async def replicate_file(source_node: str, ip_id: str, requester_wallet: str):
    """
    Replicate file from source node to this node
    License verification at both ends!
    """

    # 1. This node verifies requester has license
    if not verify_license_on_node(ip_id, requester_wallet):
        raise PermissionError("Requester lacks license for this IP Asset")

    # 2. Request file from source node
    async with grpc.aio.insecure_channel(source_node) as channel:
        stub = sequentias_pb2_grpc.NodeStub(channel)

        # Source node ALSO verifies (defense in depth)
        response = await stub.ReplicateFile(
            sequentias_pb2.ReplicationRequest(
                ip_id=ip_id,
                requester_node=THIS_NODE_ID,
                requester_wallet=requester_wallet
            )
        )

    # 3. Verify checksum
    if hashlib.sha256(response.data).hexdigest() != response.checksum:
        raise IntegrityError("File corrupted during transfer")

    # 4. Store locally with license metadata
    await local_vault.store(
        ip_id=ip_id,
        data=response.data,
        license_verified_for=requester_wallet,
        replicated_from=source_node,
        replicated_at=time.time()
    )
```

---

## üìä Performance Comparison

### Current Centralized Architecture:
```
Researcher in Mexico ‚Üí California Server (200ms latency)
  ‚Üì
Download 150GB WGS file (30 minutes on 1 Gbps)
  ‚Üì
Analyze locally (2 hours)
  ‚Üì
Total: 2.5 hours
```

### Sequentias Distributed Architecture:
```
Researcher in Mexico ‚Üí Sequentias Network
  ‚Üì
Routes to Mexico City node (20ms latency)
  ‚Üì
Mexico node caches file from California (one-time, 30 min)
  ‚Üì
Analyze on Mexico node (2 hours, but starts immediately)
  ‚Üì
Results stream back (instant)
  ‚Üì
Total first run: 2.5 hours (same as centralized)
Total subsequent runs: 2 hours (30min saved, file already cached!)

Next researcher in Mexico using same file: 2 hours (no download!)
```

### GPU Jobs:
```
AlphaGenome job from Tokyo on EU data:

Centralized:
  Tokyo ‚Üí California server ‚Üí Fetch EU data ‚Üí GPU analyze ‚Üí Return
  Total: 3 hours (200ms latency + data transfer + compute)

Sequentias:
  Tokyo ‚Üí Singapore node ‚Üí Fetch EU data ‚Üí GPU analyze ‚Üí Return
  Total: 2 hours (50ms latency + optimized data transfer + same compute)
  Savings: 33% faster!
```

---

## üéØ THE ANSWER: Build BOTH FUSE + BioNFS

### FUSE (Client Layer):
‚úÖ **Purpose**: User experience - mount files locally
‚úÖ **Location**: `/home/ubuntu/web3fuse/` (enhance existing)
‚úÖ **Implementation**: C library with Sequentias integration
‚úÖ **User sees**: `~/genomics/0xCCe1.../file.vcf` (appears local)
‚úÖ **Actually**: File streamed from optimal Sequentias node

### BioNFS Protocol (Network Layer):
‚úÖ **Purpose**: Node-to-node communication
‚úÖ **Location**: `/home/ubuntu/sequentias_node/bionfs_server/` (new)
‚úÖ **Implementation**: FastAPI + gRPC + WebSocket
‚úÖ **Handles**: Job routing, data replication, streaming

### Sequentias Registry (Orchestration Layer):
‚úÖ **Purpose**: Smart routing & resource allocation
‚úÖ **Location**: Story Protocol smart contract (on-chain)
‚úÖ **Implementation**: Solidity contract + Python orchestrator
‚úÖ **Decides**: Which node executes which job

---

## üöÄ Implementation Priority

### Month 1: Foundation
- ‚úÖ BioNFS protocol specification (done)
- ‚úÖ Basic FastAPI server
- ‚úÖ FUSE client with network support
- ‚úÖ Simple node registration

### Month 2: Distribution
- ‚úÖ Sequentias smart contract
- ‚úÖ Orchestration logic (node scoring)
- ‚úÖ Data replication protocol
- ‚úÖ 3 test nodes (Mexico, California, EU)

### Month 3: Production
- ‚úÖ Full node capabilities (all APIs)
- ‚úÖ GPU node setup (Clara, AlphaGenome)
- ‚úÖ Fault tolerance (retry logic)
- ‚úÖ 10+ production nodes globally

### Month 4: Optimization
- ‚úÖ Edge caching optimization
- ‚úÖ Cost minimization
- ‚úÖ Performance benchmarks
- ‚úÖ Open node registration (anyone can run a node!)

---

## üí° Why This is Revolutionary

1. **True Decentralization**: Not just data, but compute too
2. **Patient Sovereignty**: Data follows patient, not server location
3. **Cost Optimization**: Jobs route to cheapest available node
4. **Fault Tolerance**: Node down? Automatically retry elsewhere
5. **Global Scale**: Add nodes anywhere in the world
6. **Open Network**: Anyone can run a Sequentias node
7. **Standards-Based**: Built on IETF NFS v4 + Story Protocol

This is not just GenoBank CLI. **This is the decentralized genomics network of the future.** üåçüß¨

